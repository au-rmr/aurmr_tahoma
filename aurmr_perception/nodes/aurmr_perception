#!/usr/bin/env python

import argparse
import cv2
from cv2 import COLOR_RGB2BGR
import numpy as np
import ros_numpy
import rospy
import message_filters

from collections import defaultdict
from sensor_msgs.msg import Image, CameraInfo, PointField, PointCloud2
from std_msgs.msg import Header
from aurmr_perception.srv import *


class ObjectBinQueue():
    def __init__(self):
        self.queue = []
    
    def put(self, bin_id):
        self.queue.append(bin_id)
    
    def get(self, remove=True):
        result = self.queue[-1]
        if remove:
            self.queue = self.queue[:-1]
        return result

    def remove(self, bin_id):
        self.queue.reverse()
        self.queue.remove(bin_id)
        self.queue.reverse()
    
    def empty(self):
        return len(self.queue) == 0


class AURMRPerceptionNode():
    def __init__(self, args):
        self.diff_threshold = args.diff_threshold
        self.latest_captures = {}
        self.points_table = {}
        self.object_bin_queues = defaultdict(ObjectBinQueue)
        self.bin_normals = {}
        self.latest_mask = None
        self.rgb_image = None
        self.visualize = args.viz

    def capture_object_callback(self, request):
        if not request.bin_id or not request.object_id:
            return CaptureObjectResponse(success=False, message="bin_id and object_id are required")

        if request.bin_id not in self.latest_captures:
            return CaptureObjectResponse(success=False, message=f"Bin {request.bin_id} has not been reset")

        last_rgb = self.latest_captures[request.bin_id]
        current_rgb = self.rgb_image

        # Get the difference of the two captured RGB images
        difference = cv2.absdiff(last_rgb, current_rgb)

        # Threshold the difference image to get the initial mask
        mask = difference.sum(axis=2) >= self.diff_threshold

        # Group the masked pixels and leave only the group with the largest area
        (numLabels, labels, stats, centroids) = cv2.connectedComponentsWithStats(mask.astype(np.uint8), 8, cv2.CV_32S)
        areas = np.array([stats[i, cv2.CC_STAT_AREA] for i in range(len(stats))])
        max_area = np.max(areas[1:])
        max_area_idx = np.where(areas == max_area)[0][0]
        mask[np.where(labels != max_area_idx)] = 0.0

        # Apply mask to the depth image
        masked_depth = self.depth_image * mask

        # Get intrinsics reported via camera_info (focal point and principal point offsets)
        fp_x = self.camera_info.K[0]
        fp_y = self.camera_info.K[4]
        ppo_x = self.camera_info.K[2]
        ppo_y = self.camera_info.K[5]

        # Convert the masked depth into a point cloud
        height, width = masked_depth.shape
        nx = np.linspace(0, width-1, width)
        ny = np.linspace(0, height-1, height)
        u, v = np.meshgrid(nx, ny)
        x = (u.flatten() - ppo_x) / fp_x
        y = (v.flatten() - ppo_y) / fp_y
        z = masked_depth.flatten() / 1000;
        x = np.multiply(x,z)
        y = np.multiply(y,z)

        x = x[np.nonzero(z)]
        y = y[np.nonzero(z)]
        z = z[np.nonzero(z)]

        if request.bin_id not in self.points_table:
            self.points_table[request.bin_id] = {}

        self.points_table[request.bin_id][request.object_id] = np.vstack((x,y,z))
        self.object_bin_queues[request.object_id].put(request.bin_id)

        self.latest_captures[request.bin_id] = current_rgb
        self.latest_mask = masked_depth

        return CaptureObjectResponse(
            success=True,
            message=f"Object {request.object_id} in bin {request.bin_id} has been captured with {x.shape[0]} points."
        )

    def get_object_callback(self, request):
        if not request.object_id:
            return GetObjectPointsResponse(success=False, message="object_id is required")
        
        if self.object_bin_queues[request.object_id].empty():
            return GetObjectPointsResponse(success=False, message=f"Object {request.object_id} was not found")

        bin_id = request.bin_id
        if not bin_id:
            bin_id = self.object_bin_queues[request.object_id].get(remove=False)

        if bin_id not in self.points_table:
            return GetObjectPointsResponse(success=False, message=f"Bin {bin_id} was not found")
        
        if request.object_id not in self.points_table[bin_id]:
            return GetObjectPointsResponse(
                success=False,
                message=f"Object {request.object_id} was not found in bin {bin_id}"
            )

        # Convert numpy points to a pointcloud message
        parent_frame = 'camera_color_frame' # TODO: translate to a common coordinate frame
        itemsize = np.dtype(np.float32).itemsize
        points = self.points_table[bin_id][request.object_id].transpose((1,0))
        points = np.hstack((points, np.ones((points.shape[0], 4))))
        num_points = points.shape[0]

        data = points.astype(np.float32).tobytes()

        fields = [PointField(
            name=n, offset=i*itemsize, datatype=PointField.FLOAT32, count=1)
            for i, n in enumerate('xyzrgba')]

        header = Header(frame_id=parent_frame, stamp=rospy.Time.now())

        pointcloud = PointCloud2(
            header=header,
            height=1,
            width=num_points,
            is_dense=False,
            is_bigendian=False,
            fields=fields,
            point_step=(itemsize * 7),
            row_step=(itemsize * 7 * num_points),
            data=data
        )

        return GetObjectPointsResponse(
            success=True,
            message=f"Points successfully retrieved for object {request.object_id} in bin {bin_id}",
            bin_id=bin_id,
            points=pointcloud
        )

    def remove_object_callback(self, request):
        if not request.bin_id or not request.object_id:
            return RemoveObjectResponse(success=False, message="bin_id and object_id are required")
        
        if request.bin_id not in self.points_table:
            return RemoveObjectResponse(success=False, message=f"Bin {request.bin_id} was not found")
        
        if request.object_id not in self.points_table[request.bin_id]:
            return RemoveObjectResponse(
                success=False,
                message=f"Object {request.object_id} was not found in bin {request.bin_id}"
            )

        del self.points_table[request.bin_id][request.object_id]
        self.object_bin_queues[request.object_id].remove(request.bin_id)
        self.latest_captures[request.bin_id] = self.rgb_image

        return RemoveObjectResponse(
            success=True,
            message=f"Object {request.object_id} was removed from bin {request.bin_id}"
        )
        
    
    def reset_callback(self, request):
        if not request.bin_id:
            return ResetBinResponse(success=False, message="bin_id is required")
        if self.rgb_image is None:
            return ResetBinResponse(success=False, message="No images have been streamed")
        
        self.latest_captures[request.bin_id] = self.rgb_image

        return ResetBinResponse(success=True, message=f"Bin {request.bin_id} has been reset")
    
    def camera_callback(self, ros_depth_image, ros_rgb_image, ros_camera_info):
        self.rgb_image = ros_numpy.numpify(ros_rgb_image)
        self.depth_image = ros_numpy.numpify(ros_depth_image)
        self.camera_info = ros_camera_info

        if self.visualize:
            rgb_im_viz = cv2.cvtColor(self.rgb_image, cv2.COLOR_RGB2BGR)
            cv2.imshow('rgb_image',rgb_im_viz)
            cv2.waitKey(1)

            bin_im_viz = np.zeros((480,640))
            if '3F' in self.latest_captures:
                bin_im_viz = self.latest_captures['3F']
                bin_im_viz = cv2.cvtColor(bin_im_viz, cv2.COLOR_RGB2BGR)
            cv2.imshow('latest_bin_capture',bin_im_viz)
            cv2.waitKey(1)

            mask_im_viz = np.zeros((480,640))
            if self.latest_mask is not None:
                mask_im_viz = self.latest_mask.astype(float)
            cv2.imshow('latest_mask',mask_im_viz)
            cv2.waitKey(1)

    def main(self):
        rospy.init_node('aurmr_perception')
        self.node_name = rospy.get_name()
        rospy.loginfo("{0} started".format(self.node_name))

        self.trigger_capture = rospy.Service('/aurmr_perception/capture_object', CaptureObject, self.capture_object_callback)
        self.trigger_remove = rospy.Service('/aurmr_perception/remove_object', RemoveObject, self.remove_object_callback)
        self.trigger_retrieve = rospy.Service('/aurmr_perception/get_object_points', GetObjectPoints, self.get_object_callback)
        self.trigger_reset = rospy.Service('/aurmr_perception/reset_bin', ResetBin, self.reset_callback)

        self.camera_depth_subscriber = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
        self.camera_rgb_subscriber = message_filters.Subscriber('/camera/color/image_raw', Image)
        self.camera_info_subscriber = message_filters.Subscriber('/camera/color/camera_info', CameraInfo)

        self.camera_synchronizer = message_filters.TimeSynchronizer([
            self.camera_depth_subscriber, self.camera_rgb_subscriber, self.camera_info_subscriber], 10)
        self.camera_synchronizer.registerCallback(self.camera_callback)
        
        self.rate = 5.0
        rate = rospy.Rate(self.rate)
 
        while not rospy.is_shutdown():
            rate.sleep()


if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description='AURMR Perception Module')
        parser.add_argument('-v', '--viz', action='store_true', default=True)
        parser.add_argument('--diff_threshold', type=int, default=140)
        args, unknown = parser.parse_known_args()
        node = AURMRPerceptionNode(args)
        node.main()
        rospy.spin()
    except KeyboardInterrupt:
        print('interrupt received, so shutting down')
