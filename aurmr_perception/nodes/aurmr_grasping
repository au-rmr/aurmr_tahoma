#!/usr/bin/env python

import argparse
import cv2
import numpy as np
import rospy
import ros_numpy
import message_filters
import threading as th

from collections import defaultdict
from sensor_msgs.msg import Image, CameraInfo, PointField, PointCloud2
from sensor_msgs.point_cloud2 import read_points
from std_msgs.msg import Header
# from aurmr_perception.srv import *
from geometry_msgs.msg import PoseStamped, Quaternion, Pose, Point
from tf_conversions import transformations

import sys
import os
os.chdir('/home/aurmr/workspaces/vatsa_ws/graspnet/pytorch_6dof-graspnet')
sys.path.insert(1, '/home/aurmr/workspaces/vatsa_ws/graspnet/pytorch_6dof-graspnet')
import grasp_estimator


import glob
import mayavi.mlab as mlab
from utils.visualization_utils import *
import mayavi.mlab as mlab
from utils import utils
from data import DataLoader


class AURMRGraspingNode():
    def __init__(self):
        align_to_bin_orientation = transformations.quaternion_from_euler(1.57, 0, 1.57)
        self.align_to_bin_quat = Quaternion(x=align_to_bin_orientation[0], y=align_to_bin_orientation[1],
                                       z=align_to_bin_orientation[2], w=align_to_bin_orientation[3])
        self.normal_vector = np.array((-1,0,0))
        self.rgb_image = None
        self.depth_image = None
        self.camera_info = None

    def make_graspnet_parser(self):
        parser = argparse.ArgumentParser(
            description='6-DoF GraspNet Demo',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('--grasp_sampler_folder',
                            type=str,
                            default='checkpoints/gan_pretrained/')
        parser.add_argument('--grasp_evaluator_folder',
                            type=str,
                            default='checkpoints/evaluator_pretrained/')
        parser.add_argument('--refinement_method',
                            choices={"gradient", "sampling"},
                            default='sampling')
        parser.add_argument('--refine_steps', type=int, default=25)

        parser.add_argument(
            '--threshold',
            type=float,
            default=0.8,
            help=
            "When choose_fn is something else than all, all grasps with a score given by the evaluator notwork less than the threshold are removed"
        )
        parser.add_argument(
            '--choose_fn',
            choices={
                "all", "better_than_threshold", "better_than_threshold_in_sequence"
            },
            default='better_than_threshold',
            help=
            "If all, no grasps are removed. If better than threshold, only the last refined grasps are considered while better_than_threshold_in_sequence consideres all refined grasps"
        )

        parser.add_argument('--target_pc_size', type=int, default=1024)
        parser.add_argument('--num_grasp_samples', type=int, default=200)
        parser.add_argument(
            '--generate_dense_grasps',
            action='store_true',
            help=
            "If enabled, it will create a [num_grasp_samples x num_grasp_samples] dense grid of latent space values and generate grasps from these."
        )

        parser.add_argument(
            '--batch_size',
            type=int,
            default=30,
            help=
            "Set the batch size of the number of grasps we want to process and can fit into the GPU memory at each forward pass. The batch_size can be increased for a GPU with more memory."
        )
        parser.add_argument('--train_data', action='store_true')
        opts, _ = parser.parse_known_args()
        if opts.train_data:
            parser.add_argument('--dataset_root_folder',
                                required=True,
                                type=str,
                                help='path to root directory of the dataset.')
        return parser

    def filterGrasps(self, grasps, grasp_scores):
        filtered_grasps = []
        filtered_scores = []
        for grasp, grasp_score in zip(grasps, grasp_scores):
            euler_grasp = transformations.euler_from_matrix(grasp[0:3, 0:3])
            norm_euler_grasp = np.linalg.norm(euler_grasp[0:1])  # Taking only x and y angles
            if norm_euler_grasp < 1:
                filtered_grasps.append(grasp)
                filtered_scores.append(grasp_score)
            # print(norm_euler_grasp)
        return filtered_grasps, filtered_scores

    def graspnet(self):
        # Depending on your numpy version you may need to change allow_pickle
        # from True to False.
        # input('Extracting depth')
        graspnet_parser = self.make_graspnet_parser()
        graspnet_args = parser.parse_args()
        grasp_sampler_args = utils.read_checkpoint_args(graspnet_args.grasp_sampler_folder)
        grasp_sampler_args.is_train = False
        grasp_evaluator_args = utils.read_checkpoint_args(
            graspnet_args.grasp_evaluator_folder)
        grasp_evaluator_args.continue_train = True
        estimator = grasp_estimator.GraspEstimator(grasp_sampler_args,
                                                   grasp_evaluator_args, args)

        depth_scale_factor = 0.001
        depth = self.depth_image * depth_scale_factor
        image = self.rgb_image

        camera_intrinsics = self.camera_info
        object_pc = np.float64(data['smoothed_object_pc'])

        object_pc_x = -np.copy(object_pc[:, 2])
        object_pc_y = -np.copy(object_pc[:, 1])
        object_pc_z = np.copy(object_pc[:, 0])

        object_pc[:, 0] = object_pc_y
        object_pc[:, 1] = object_pc_x
        object_pc[:, 2] = object_pc_z

        generated_grasps, generated_scores = estimator.generate_and_refine_grasps(object_pc)

        filtered_grasps, filtered_scores = self.filterGrasps(generated_grasps, generated_scores)

        sorted_grasp_index = sorted(range(len(filtered_scores)), key=filtered_scores.__getitem__)
        sorted_generated_grasps = [filtered_grasps[i] for i in reversed(sorted_grasp_index)]
        sorted_generated_scores = [filtered_scores[i] for i in reversed(sorted_grasp_index)]

        return sorted_generated_grasps[0]

    def key_capture_thread(a_list):
        input('Press enter to continue...')
        a_list.append(True)

    def pc_publish(self, pts):
        pub = rospy.Publisher('ObjectPC', PointCloud2, queue_size=10)
        rate = rospy.Rate(1)
        a_list = []
        th.Thread(target=self.key_capture_thread, args=(), name='key_capture_thread', daemon=True).start()
        while not a_list:
            pub.publish(pts)
            rate.sleep()

    def pose_publish(self, pose_stamped):
        pub = rospy.Publisher('GraspPose', PoseStamped, queue_size=10)
        rate = rospy.Rate(1)
        a_list = []
        th.Thread(target=self.key_capture_thread, args=(), name='key_capture_thread', daemon=True).start()
        while not a_list:
            pub.publish(pose_stamped)
            rate.sleep()

    def save_points(self):
        np.savez('/home/enigma/catkin_ws/trial.npz', image=self.rgb_image, 
                            depth=self.depth_image, 
                            intrinsics_matrix=self.camera_info,
                            smoothed_object_pc = self.object_pointcloud)
        print('Points Saved')

    def grasping_callback(self, request):
        # get the average of the pointclouds

        # self.pc_publish(request.points)

        self.object_pointcloud = ros_numpy.numpify(request.points)
        self.object_pointcloud = np.stack([self.object_pointcloud['x'],
                                            self.object_pointcloud['y'],
                                            self.object_pointcloud['z']], axis=1)


        ros_depth_image = rospy.wait_for_message('/camera_lower_left/aligned_depth_to_color/image_raw', Image, timeout=1)
        ros_rgb_image = rospy.wait_for_message('/camera_lower_left/color/image_raw', Image, timeout=1)
        ros_camera_info = rospy.wait_for_message('/camera_lower_left/color/camera_info', CameraInfo, timeout=1)

        self.rgb_image = ros_numpy.numpify(ros_rgb_image)
        self.depth_image = ros_numpy.numpify(ros_depth_image)
        self.camera_info = np.array(ros_camera_info.K).reshape((3,3))

        self.save_points()

        if False:
            ################# hack for the first pick-up:#################
            # compute the average point of the pointcloud
            center = np.mean(self.object_pointcloud, axis = 0)
            # NOTE(nickswalker,4-29-22): Hack to compensate for the chunk of points that don't get observed
            # due to the lip of the bin
            center[2] -= 0.02
            # get the vector
            extention_dir = self.normal_vector
            dist = request.dist_th
            position = dist * extention_dir + center # center and extention_dir should be under the same coordiante!
            # pick a orientation
            pose_pool = [-self.normal_vector, np.array([0, 0, np.pi/2])] # assuming the robot keeps the same approaching vector

            # pick a gripper closing distance
            gripper_pool = [0, 0.01, 0.02, 0.05]

            gripper_dist = gripper_pool[request.grasp_id]

            # todo after the first pick-up:
            # train GraspNet that is given the pointclouds, outputs the grasping candidates, GraspNet doesn't output gripper_dist,
            # assuming the gripper is flexible enough, we can just set a hard-code distance to close the finger.
            # only get the most confident grasp
            # might need to consider motion planning collision checking etc.
            grasp_pose = PoseStamped(header=request.points.header, pose=Pose(position=Point(x=position[0], y=position[1], z=position[2]),orientation=self.align_to_bin_quat))

        grasp_pose_se3 = self.graspnet()
        grasp_point = Point(x=grasp_pose_se3[3,0], y=grasp_pose_se3[3,1], z=grasp_pose_se3[3,2])

        grasp_orientation_quat = transformations.quaternion_from_matrix(grasp_pose_se3[:3,:3])
        grasp_orientation = Quaternion(x=grasp_orientation_quat[0], y=grasp_orientation_quat[1],
                                       z=grasp_orientation_quat[2], w=grasp_orientation_quat[3])
        
        grasp_pose = Pose(position=grasp_point,
                                orientation=grasp_orientation)

        grasp_pose_stamped = PoseStamped(header=request.points.header, 
                                pose=grasp_pose)

        self.pose_publish(grasp_pose_stamped)

        return GraspPoseResponse(success=True, message=f"Grasping Pose has been set", # The function name will be different
                                pose = grasp_pose_stamped,
                                grasp = 0.02)      #gripper_dist


    def main(self):
        print('Grasping node running')
        rospy.init_node('aurmr_grasping')
        self.node_name = rospy.get_name()
        rospy.loginfo("{0} started".format(self.node_name))

        self.trigger_grasp = rospy.Service('/aurmr_perception/init_grasp', GraspPose, self.grasping_callback)

        self.rate = 5.0
        rate = rospy.Rate(self.rate)

        while not rospy.is_shutdown():
            rate.sleep()


if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description='AURMR Perception Module')
        parser.add_argument('-v', '--viz', action='store_true', default=True)
        parser.add_argument('--diff_threshold', type=int, default=140)
        args, unknown = parser.parse_known_args()
        node = AURMRGraspingNode()
        node.main()
        rospy.spin()
    except KeyboardInterrupt:
        print('interrupt received, so shutting down')