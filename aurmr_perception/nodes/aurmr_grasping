#!/usr/bin/env python

import argparse
import cv2
import numpy as np
import rospy
import ros_numpy
import message_filters
import threading as th

from collections import defaultdict
from sensor_msgs.msg import Image, CameraInfo, PointField, PointCloud2
from sensor_msgs.point_cloud2 import read_points
from std_msgs.msg import Header
from aurmr_perception.srv import *
from geometry_msgs.msg import PoseStamped, Quaternion, Pose, Point
from tf_conversions import transformations



class AURMRGraspingNode():
    def __init__(self):
        align_to_bin_orientation = transformations.quaternion_from_euler(1.57, 0, 1.57)
        self.align_to_bin_quat = Quaternion(x=align_to_bin_orientation[0], y=align_to_bin_orientation[1],
                                       z=align_to_bin_orientation[2], w=align_to_bin_orientation[3])
        self.normal_vector = np.array((-1,0,0))
        self.rgb_image = None
        self.depth_image = None
        self.camera_info = None

    def key_capture_thread(a_list):
        input('Press enter to continue...')
        a_list.append(True)

    def pcPublish(self, pts):
        pub = rospy.Publisher('ObjectPC', PointCloud2, queue_size=10)
        rate = rospy.Rate(1)
        a_list = []
        th.Thread(target=self.key_capture_thread, args=(), name='key_capture_thread', daemon=True).start()
        while not a_list:
            pub.publish(pts)
            rate.sleep()

    def savePoints(self):
        np.savez('/home/enigma/catkin_ws/trial.npz', image=self.rgb_image, 
                            depth=self.depth_image, 
                            intrinsics_matrix=self.camera_info,
                            smoothed_object_pc = self.object_pointcloud)
        print('Points Saved')

    def grasping_callback(self, request):
        # get the average of the pointclouds

        # self.pcPublish(request.points)

        self.object_pointcloud = ros_numpy.numpify(request.points)
        self.object_pointcloud = np.stack([self.object_pointcloud['x'],
                                            self.object_pointcloud['y'],
                                            self.object_pointcloud['z']], axis=1)


        ros_depth_image = rospy.wait_for_message('/stand_camera/aligned_depth_to_color/image_raw', Image, timeout=1)
        ros_rgb_image = rospy.wait_for_message('/stand_camera/color/image_raw', Image, timeout=1)
        ros_camera_info = rospy.wait_for_message('/stand_camera/color/camera_info', CameraInfo, timeout=1)

        self.rgb_image = ros_numpy.numpify(ros_rgb_image)
        self.depth_image = ros_numpy.numpify(ros_depth_image)
        self.camera_info = np.array(ros_camera_info.K).reshape((3,3))

        self.savePoints()

        ################# hack for the first pick-up:#################
        # compute the average point of the pointcloud
        center = np.mean(self.object_pointcloud, axis = 0)
        # NOTE(nickswalker,4-29-22): Hack to compensate for the chunk of points that don't get observed
        # due to the lip of the bin
        center[2] -= 0.02
        # get the vector
        extention_dir = self.normal_vector
        dist = request.dist_th
        position = dist * extention_dir + center # center and extention_dir should be under the same coordiante!
        # pick a orientation
        pose_pool = [-self.normal_vector, np.array([0, 0, np.pi/2])] # assuming the robot keeps the same approaching vector

        # pick a gripper closing distance
        gripper_pool = [0, 0.01, 0.02, 0.05]

        gripper_dist = gripper_pool[request.grasp_id]

        # todo after the first pick-up:
        # train GraspNet that is given the pointclouds, outputs the grasping candidates, GraspNet doesn't output gripper_dist,
        # assuming the gripper is flexible enough, we can just set a hard-code distance to close the finger.
        # only get the most confident grasp
        # might need to consider motion planning collision checking etc.
        grasp_pose = PoseStamped(header=request.points.header, pose=Pose(position=Point(x=position[0], y=position[1], z=position[2]),orientation=self.align_to_bin_quat))

        return GraspPoseResponse(success=True, message=f"Grasping Pose has been set", # The function name will be different
                                pose = grasp_pose,
                                grasp = gripper_dist)


    def main(self):
        print('Grasping node running')
        rospy.init_node('aurmr_grasping')
        self.node_name = rospy.get_name()
        rospy.loginfo("{0} started".format(self.node_name))

        self.trigger_grasp = rospy.Service('/aurmr_perception/init_grasp', GraspPose, self.grasping_callback)

        self.rate = 5.0
        rate = rospy.Rate(self.rate)

        while not rospy.is_shutdown():
            rate.sleep()


if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description='AURMR Perception Module')
        parser.add_argument('-v', '--viz', action='store_true', default=True)
        parser.add_argument('--diff_threshold', type=int, default=140)
        args, unknown = parser.parse_known_args()
        node = AURMRGraspingNode()
        node.main()
        rospy.spin()
    except KeyboardInterrupt:
        print('interrupt received, so shutting down')